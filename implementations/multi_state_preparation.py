# -*- coding: utf-8 -*-
"""Multi-state prep.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p6vMCroXV1gMofvb9qm8YJrh3B9hRqdW
"""

!pip install torch
!pip install pennylane

"""Importing all libraries"""

import pennylane as qml
import numpy as np
import torch
from torch.autograd import Variable
import math
import matplotlib.pyplot as plt
from torchvision import datasets
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
np.random.seed(42)

"""Importing required dataset to be encoded ; here MNIST is being used"""

# Load any dataset, here MNIST is being used
mnist = datasets.MNIST(root='./data', train=True, download=True)

# Get first 20 images
images = []
for i in range(16):
    image, _ = mnist[i]
    images.append(np.array(image).flatten())  # Flatten each 28x28 image to 784 vector

# Convert to numpy array
X = np.array(images)

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply PCA with x components
pca = PCA(n_components=16)
X_pca = pca.fit_transform(X_scaled)

# Apply MinMax scaling to ensure all values are positive (between 0 and 1)
minmax_scaler = MinMaxScaler(feature_range=(0, 1))
X_positive = minmax_scaler.fit_transform(X_pca)

# Convert to tensor
state = torch.tensor(X_positive, dtype=torch.float32)

"""1. All handwritten datasets used in earlier testing to form the model
2. Normalizing the data row-wise
"""

states = torch.tensor(state, requires_grad=False)

for i in range(len(states)):
  states[i] = states[i] / torch.sqrt(torch.sum(states[i]**2))

"""Defining number of layers to be used -- CHANGE IF NEEDED"""

numOfStates = len(states)

# number of qubits needed
nr_qubits = max(int(np.ceil(np.log2(len(states[0])))), int(np.ceil(np.log2(len(states)))))

# number of ancilla
if (int(np.ceil(np.log2(len(states)))) > int(np.ceil(np.log2(len(states[0]))))):
  ancilla = int(np.ceil(np.log2(len(states)))) - int(np.ceil(np.log2(len(states[0]))))
else:
  ancilla = 0

# number of layers in the circuit
nr_layers = 12

# # randomly initialize parameters from a normal distribution
params = np.random.normal(0, np.pi, (nr_qubits, nr_layers, 2))
params = Variable(torch.tensor(params), requires_grad=True)

"""Next 2 blocks of code: preparing the circuit"""

# a layer of the circuit ansatz
def layer(params, j):
    for i in range(nr_qubits):
        # qml.RX(params[i, j, 0], wires=i)
        qml.RY(params[i, j, 0], wires=i)
        # qml.RZ(params[i, j, 1], wires=i)

    for i in range(nr_qubits):
      if(i!=nr_qubits-1):
        qml.CRX(params[i, j, 1], wires=[i, i + 1])
      else:
        qml.CRX(params[i, j, 1], wires=[i, 0])

# ---- CHANGE IF NUMBER OF QUBITS, ANCILLA, STATES CHANGES --- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
dev = qml.device("default.qubit", wires=nr_qubits)

@qml.qnode(dev, interface="torch")
def circuit(params, case):

    # preparing initial state according to number of states
    # ---- CHANGE IF NUMBER OF QUBITS, STATES CHANGES --- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    binary = format(case, f'0{nr_qubits}b')
    for qubit_idx, bit in enumerate(binary):
        if bit == '1':
            qml.PauliX(wires=qubit_idx)

    qml.Barrier(wires=range(nr_qubits))

    for i in range(nr_qubits):
      qml.Hadamard(wires=i)

    # # repeatedly apply each layer in the circuit
    for j in range(nr_layers):
        layer(params, j)

    for i in range(nr_qubits):
      qml.Hadamard(wires=i)

    return qml.probs(wires=range(nr_qubits))
    # return qml.state()

"""To get fidelity values for plots"""

def fidelity_val(params, k):
  circuit_output = circuit(params, k)**(0.5)
  target_state = states[k]
  target_state = target_state.type(circuit_output.dtype)

  fidelity = 0

  if(ancilla > 0):
    sum=0
    first_half, second_half = torch.split(circuit_output, circuit_output.shape[0] // 2)
    # Calculate a and b
    a = torch.sqrt(torch.sum(first_half**2))
    # Calculate |ψ'⟩ and |E⟩
    psi_dash = torch.tensor(first_half/a)
    # Calculate innerproduct fidelity
    fidelity = torch.abs(torch.dot(psi_dash.conj(), target_state))**2

  else:
    # Calculate innerproduct fidelity
    fidelity = torch.abs(torch.dot(circuit_output.conj(), target_state))**2

  return(fidelity.detach().numpy())

"""Defining Fidelity cost for non-ancilla and ancilla circuits"""

def fidelity_cost(params):
  cost=0
  max=0

  for k in range(numOfStates):

        circuit_output = circuit(params, k)**(0.5)
        target_state = states[k]
        target_state = target_state.type(circuit_output.dtype)

        fidelity = 0
        state_cost = 0

        if(ancilla > 0):
          sum=0
          first_half, second_half = torch.split(circuit_output, circuit_output.shape[0] // 2)
          # Calculate a and b
          a = torch.sqrt(torch.sum(first_half**2))
          # Calculate |ψ'⟩ and |E⟩
          psi_dash = torch.tensor(first_half/a)
          # Calculate innerproduct fidelity
          fidelity = torch.abs(torch.dot(psi_dash.conj(), target_state))**2
          # Convert fidelity to cost
          state_cost = 0.2*(1 - (torch.abs(a))**2) + 0.8*(1 - (fidelity))

        else:
          # Calculate innerproduct fidelity
          fidelity = torch.abs(torch.dot(circuit_output.conj(), target_state))**2
          # Convert fidelity to cost
          state_cost = 1 - fidelity

        if(state_cost>max):
          max=state_cost

        # Add to total cost
        cost += state_cost

  # Normalize cost by number of states
  avg_cost = cost / numOfStates

  return max

"""Defining MSE cost for non-anciila and ancilla circuits (CHANGE IF ANCILLA qubits used)"""

# def mse_cost(params):
#   cost=0

#   for k in range(numOfStates):

#         circuit_output = circuit(params, k)
#         target_state = states[k]
#         target_state = target_state.type(circuit_output.dtype)

#         # Calculate a and b
#         a = torch.sqrt(torch.abs(circuit_output[0])**2 + torch.abs(circuit_output[1])**2 + torch.abs(circuit_output[2])**2 + torch.abs(circuit_output[3])**2)

#         # Calculate |ψ'⟩ and |E⟩
#         psi_dash = torch.tensor([circuit_output[0]/a, circuit_output[1]/a, circuit_output[2]/a, circuit_output[3]/a])

#         # Calculate innerproduct fidelity
#         mse_cost = (torch.sum((psi_dash.real - target_state.real)**2)) / numOfStates

#         # Convert mse_cost to cost
#         state_cost = 0.2*(1 - (torch.abs(a))**2) + 0.8*(mse_cost)

#         # Add to total cost
#         cost += state_cost

#   # Normalize cost by number of states
#   avg_cost = cost / numOfStates

#   return avg_cost

"""Running the model for the suitable cost ; here Fidelity cost is being used"""

loss_values = []
iteration_values = []
state_fidelities = [[] for _ in range(numOfStates)]

# set up the optimizer - RPROP OR ADAMW
opt = torch.optim.Adam([params])

# number of steps in the optimization routine
steps = 1500

# the final stage of optimization isn't always the best, so we keep track of
# the best parameters along the way
best_cost = fidelity_cost(params)
best_params = np.zeros((nr_qubits, nr_layers, 2))
loss_values.append(best_cost.detach().numpy())
iteration_values.append(0)
for k in range(numOfStates):
    fidel = fidelity_val(params, k)
    state_fidelities[k].append(fidel.item())

print("Cost after 0 steps is {:.4f}".format(fidelity_cost(params)))

# optimization begins
for n in range(steps):
    opt.zero_grad()
    loss = fidelity_cost(params)
    loss.backward()
    opt.step()

    loss_values.append(loss.item())  # Get the scalar value of the loss as a Python number
    iteration_values.append(n + 1)

    for k in range(numOfStates):
      fidel = fidelity_val(params, k)
      state_fidelities[k].append(fidel.item())

    # keeps track of best parameters
    if loss < best_cost:
        best_cost = loss
        best_params = params

    # Keep track of progress every 10 steps
    if n % 10 == 9 or n == steps - 1:
        print("Cost after {} steps is {:.4f}".format(n + 1, loss))

for k in range(numOfStates):
    plt.plot(state_fidelities[k], label=f'State {k}')
plt.xlabel('Step')
plt.ylabel('Fidelity')
plt.title('State Fidelities Over Time')
plt.legend()
plt.show()

"""To get the states produced if Fidelity cost is used
(Also CHANGE IF ANCILLA qubits are used)

"""

for k in range(numOfStates):
    circuit_output = circuit(best_params, k)**(0.5)
    target_state = states[k]
    target_state = target_state.type(circuit_output.dtype)
    # circuit_output /= torch.linalg.norm(circuit_output)

    # a = torch.sqrt((circuit_output[0])**2 + (circuit_output[1])**2 + (circuit_output[2])**2 + (circuit_output[3])**2)
    # psi_dash = torch.tensor([circuit_output[0]/a, circuit_output[1]/a, circuit_output[2]/a, circuit_output[3]/a])

    print(circuit_output)
    print(target_state)
    # fidelity = (torch.abs(torch.dot(psi_dash.conj(), target_state))**(2))
    fidelity = qml.math.fidelity_statevector(circuit_output, target_state)
    print(fidelity)
    # print(0.2*(1 - (torch.abs(a))**2) + 0.8*(1 - (fidelity)))
    print(1-fidelity)

    print()

print(fidelity_cost(best_params))

"""To get the states produced if MSE cost is used"""

# for k in range(numOfStates):
#     circuit_output = circuit(best_params, k)
#     target_state = states[k]
#     target_state = target_state.type(circuit_output.dtype)

#     a = torch.sqrt(torch.abs(circuit_output[0])**2 + torch.abs(circuit_output[1])**2 + torch.abs(circuit_output[2])**2 + torch.abs(circuit_output[3])**2)
#     psi_dash = torch.tensor([circuit_output[0]/a, circuit_output[1]/a, circuit_output[2]/a, circuit_output[3]/a])

#     print(psi_dash)
#     print(target_state)
#     print(a)
#     # Calculate mse_cost
#     mse_cost = (torch.sum((psi_dash.real - target_state.real)**2)) / numOfStates
#     print(mse_cost)
#     print(0.2*(1 - (torch.abs(a))**2) + 0.8*(mse_cost))

#     print()

# print(mse_real_cost(best_params))

drawer = qml.draw(circuit)
print(drawer(best_params, 19))

